{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tushar-rancy/ensemble_learning-Assignment/blob/main/ensemble_learning_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d18240c4",
      "metadata": {
        "id": "d18240c4"
      },
      "source": [
        "## Theoretical Questions\n",
        "\n",
        "**1. Can we use Bagging for regression problems?**\n",
        "\n",
        "Yes, Bagging can be used for regression tasks. The BaggingRegressor in scikit-learn is designed for such use cases. It aggregates the predictions from multiple regressors by averaging them.\n",
        "\n",
        "**2. What is the difference between multiple model training and single model training?**\n",
        "\n",
        "Single model training involves training a single algorithm, whereas multiple model training (ensemble learning) combines predictions from several models to improve accuracy and robustness.\n",
        "\n",
        "**3. Explain the concept of feature randomness in Random Forest.**\n",
        "\n",
        "Random Forest introduces feature randomness by selecting a random subset of features at each split, which helps to de-correlate the trees and reduce overfitting.\n",
        "\n",
        "**4. What is OOB (Out-of-Bag) Score?**\n",
        "\n",
        "The OOB score is an internal error estimate of a Random Forest model. It's calculated using the data not included in the bootstrap sample for each tree.\n",
        "\n",
        "**5. How can you measure the importance of features in a Random Forest model?**\n",
        "\n",
        "Feature importance can be measured by how much each feature decreases the impurity in a tree. Scikit-learn provides `feature_importances_` attribute for this.\n",
        "\n",
        "**6. Explain the working principle of a Bagging Classifier.**\n",
        "\n",
        "Bagging Classifier creates multiple subsets of the original dataset using bootstrapping, trains a base estimator on each, and aggregates predictions via voting.\n",
        "\n",
        "**7. How do you evaluate a Bagging Classifierâ€™s performance?**\n",
        "\n",
        "You can evaluate it using accuracy, precision, recall, F1-score, cross-validation, or confusion matrix.\n",
        "\n",
        "**8. How does a Bagging Regressor work?**\n",
        "\n",
        "It trains multiple regressors on bootstrapped subsets and averages their predictions to reduce variance.\n",
        "\n",
        "**9. What is the main advantage of ensemble techniques?**\n",
        "\n",
        "Improved performance, reduced variance, and increased robustness.\n",
        "\n",
        "**10. What is the main challenge of ensemble methods?**\n",
        "\n",
        "Increased computational cost and reduced interpretability.\n",
        "\n",
        "**11. Explain the key idea behind ensemble techniques.**\n",
        "\n",
        "Combine predictions from multiple models to improve generalization and reduce overfitting.\n",
        "\n",
        "**12. What is a Random Forest Classifier?**\n",
        "\n",
        "An ensemble of decision trees where each tree is trained on a bootstrap sample with random feature selection at each split.\n",
        "\n",
        "**13. What are the main types of ensemble techniques?**\n",
        "\n",
        "Bagging, Boosting, Stacking, and Voting.\n",
        "\n",
        "**14. What is ensemble learning in machine learning?**\n",
        "\n",
        "A technique that combines predictions from multiple models to improve accuracy and robustness.\n",
        "\n",
        "**15. When should we avoid using ensemble methods?**\n",
        "\n",
        "When interpretability and simplicity are more important, or for small datasets.\n",
        "\n",
        "**16. How does Bagging help in reducing overfitting?**\n",
        "\n",
        "By averaging multiple models trained on bootstrapped samples, Bagging reduces variance.\n",
        "\n",
        "**17. Why is Random Forest better than a single Decision Tree?**\n",
        "\n",
        "It reduces overfitting by averaging multiple trees.\n",
        "\n",
        "**18. What is the role of bootstrap sampling in Bagging?**\n",
        "\n",
        "Bootstrap sampling creates different subsets for training individual models, helping reduce variance.\n",
        "\n",
        "**19. What are some real-world applications of ensemble techniques?**\n",
        "\n",
        "Spam detection, fraud detection, credit scoring, medical diagnosis, and recommendation systems.\n",
        "\n",
        "**20. What is the difference between Bagging and Boosting?**\n",
        "\n",
        "Bagging trains models in parallel to reduce variance, while Boosting trains models sequentially to reduce bias.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bc1f6b0",
      "metadata": {
        "id": "7bc1f6b0"
      },
      "source": [
        "## Practical Questions\n",
        "\n",
        "1. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy\n",
        "2. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)\n",
        "3. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores\n",
        "4. Train a Random Forest Regressor and compare its performance with a single Decision Tree\n",
        "5. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier\n",
        "6. Train a Bagging Classifier using SVM as a base estimator and print accuracy\n",
        "7. Train a Random Forest Classifier with different numbers of trees and compare accuracy\n",
        "8. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score\n",
        "9. Train a Random Forest Regressor and analyze feature importance scores\n",
        "10. Train an ensemble model using both Bagging and Random Forest and compare accuracy\n",
        "11. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV\n",
        "12. Train a Bagging Regressor with different numbers of base estimators and compare performance\n",
        "13. Train a Random Forest Classifier and analyze misclassified samples\n",
        "14. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier\n",
        "15. Train a Random Forest Classifier and visualize the confusion matrix\n",
        "16. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy\n",
        "17. Train a Random Forest Classifier and print the top 5 most important features\n",
        "18. Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score\n",
        "19. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy\n",
        "20. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance\n",
        "21. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score\n",
        "22. Train a Bagging Classifier and evaluate its performance using cross-validation\n",
        "23. Train a Random Forest Classifier and plot the Precision-Recall curve\n",
        "24. Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy\n",
        "25. Train a Bagging Regressor with different levels of bootstrap samples and compare performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23c7bc54",
      "metadata": {
        "id": "23c7bc54"
      },
      "outputs": [],
      "source": [
        "# Bagging Classifier Example\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=0)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}